{"cells":[{"cell_type":"markdown","source":["# streamlit ë°– ì½”ë©ì—ì„œ ë¯¸ë¦¬ ì‹¤í–‰"],"metadata":{"id":"o2n4tBh-wtbC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q streamlit\n","!npm install localtunnel\n","!pip install --upgrade -q accelerate bitsandbytes\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install sentence_transformers\n","!pip install streamlit-folium"],"metadata":{"id":"6FM2Ybz37SM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731381318276,"user_tz":-540,"elapsed":30822,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}},"outputId":"c980b83f-110a-497a-f9ec-c1d6097a4451"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\u001b[K\u001b[?25h\n","up to date, audited 23 packages in 452ms\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n","\n","To address all issues (including breaking changes), run:\n","  npm audit fix --force\n","\n","Run `npm audit` for details.\n","Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-271cpeoc\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-271cpeoc\n","  Resolved https://github.com/huggingface/transformers.git to commit 33eef992503689ba1af98090e26d3e98865b2a9b\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2024.8.30)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.47.0.dev0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n","Collecting streamlit-folium\n","  Downloading streamlit_folium-0.23.1-py3-none-any.whl.metadata (414 bytes)\n","Requirement already satisfied: streamlit>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-folium) (1.40.1)\n","Requirement already satisfied: folium!=0.15.0,>=0.13 in /usr/local/lib/python3.10/dist-packages (from streamlit-folium) (0.18.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from streamlit-folium) (3.1.4)\n","Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from streamlit-folium) (0.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium!=0.15.0,>=0.13->streamlit-folium) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium!=0.15.0,>=0.13->streamlit-folium) (2.32.3)\n","Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium!=0.15.0,>=0.13->streamlit-folium) (2024.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->streamlit-folium) (3.0.2)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=1.13.0->streamlit-folium) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (8.1.7)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (24.1)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (10.4.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (3.20.3)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (17.0.0)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (4.12.2)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (6.3.3)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.13.0->streamlit-folium) (6.0.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (0.12.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.13.0->streamlit-folium) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.13.0->streamlit-folium) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.13.0->streamlit-folium) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.13.0->streamlit-folium) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium!=0.15.0,>=0.13->streamlit-folium) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium!=0.15.0,>=0.13->streamlit-folium) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium!=0.15.0,>=0.13->streamlit-folium) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium!=0.15.0,>=0.13->streamlit-folium) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=1.13.0->streamlit-folium) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=1.13.0->streamlit-folium) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.13.0->streamlit-folium) (5.0.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit-folium) (0.20.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.13.0->streamlit-folium) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.13.0->streamlit-folium) (1.16.0)\n","Downloading streamlit_folium-0.23.1-py3-none-any.whl (327 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: streamlit-folium\n","Successfully installed streamlit-folium-0.23.1\n"]}]},{"cell_type":"code","source":["!npm audit fix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f9VJMagIcVv","executionInfo":{"status":"ok","timestamp":1731377356614,"user_tz":-540,"elapsed":1435,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}},"outputId":"a8624585-1110-4c6a-8a67-ccf863f370c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25h\n","up to date, audited 60 packages in 744ms\n","\n","5 packages are looking for funding\n","  run `npm fund` for details\n","\n","\u001b[1m# npm audit report\u001b[22m\n","\n","\u001b[1maxios\u001b[22m  <=0.27.2\n","Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n","\u001b[1mAxios vulnerable to Server-Side Request Forgery\u001b[22m - https://github.com/advisories/GHSA-4w2v-q235-vp99\n","\u001b[1maxios Inefficient Regular Expression Complexity vulnerability\u001b[22m - https://github.com/advisories/GHSA-cph5-m8f7-6c5x\n","\u001b[1mAxios Cross-Site Request Forgery Vulnerability\u001b[22m - https://github.com/advisories/GHSA-wf5p-g6vw-rhxx\n","Depends on vulnerable versions of \u001b[1mfollow-redirects\u001b[22m\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@2.0.2, which is a breaking change\n","\u001b[2mnode_modules/axios\u001b[22m\n","  \u001b[1mlocaltunnel\u001b[22m  1.9.0 - 2.0.0\n","  Depends on vulnerable versions of \u001b[1maxios\u001b[22m\n","  Depends on vulnerable versions of \u001b[1mdebug\u001b[22m\n","  Depends on vulnerable versions of \u001b[1myargs\u001b[22m\n","  \u001b[2mnode_modules/localtunnel\u001b[22m\n","\n","\u001b[1mdebug\u001b[22m  4.0.0 - 4.3.0\n","\u001b[1mRegular Expression Denial of Service in debug\u001b[22m - https://github.com/advisories/GHSA-gxpj-cx7g-858c\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@2.0.2, which is a breaking change\n","\u001b[2mnode_modules/debug\u001b[22m\n","\n","\u001b[1mfollow-redirects\u001b[22m  <=1.15.5\n","Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n","\u001b[1mExposure of Sensitive Information to an Unauthorized Actor in follow-redirects\u001b[22m - https://github.com/advisories/GHSA-pw2r-vq6v-hr8c\n","\u001b[1mExposure of sensitive information in follow-redirects\u001b[22m - https://github.com/advisories/GHSA-74fj-2j2h-c42q\n","\u001b[1mFollow Redirects improperly handles URLs in the url.parse() function\u001b[22m - https://github.com/advisories/GHSA-jchw-25xp-jwwc\n","\u001b[1mfollow-redirects' Proxy-Authorization header kept across hosts\u001b[22m - https://github.com/advisories/GHSA-cxjh-pqwp-8mfp\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@2.0.2, which is a breaking change\n","\u001b[2mnode_modules/follow-redirects\u001b[22m\n","\n","\u001b[1myargs-parser\u001b[22m  <=5.0.0\n","Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n","\u001b[1myargs-parser Vulnerable to Prototype Pollution\u001b[22m - https://github.com/advisories/GHSA-p9pc-299p-vxgp\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@2.0.2, which is a breaking change\n","\u001b[2mnode_modules/yargs-parser\u001b[22m\n","  \u001b[1myargs\u001b[22m  4.0.0-alpha1 - 7.0.0-alpha.3 || 7.1.1\n","  Depends on vulnerable versions of \u001b[1myargs-parser\u001b[22m\n","  \u001b[2mnode_modules/yargs\u001b[22m\n","\n","\u001b[31m\u001b[1m6\u001b[22m\u001b[39m vulnerabilities (1 \u001b[1mlow\u001b[22m, 3 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m, 2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m)\n","\n","To address all issues (including breaking changes), run:\n","  npm audit fix --force\n"]}]},{"cell_type":"markdown","source":["# config.py"],"metadata":{"id":"FjTbTWkhENyp"}},{"cell_type":"code","source":["%%writefile config.py\n","\n","import streamlit as st\n","import pandas as pd\n","from PIL import Image, ImageOps\n","import torch\n","from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.neighbors import NearestNeighbors\n","import itertools\n","import requests\n","import urllib.parse\n","import requests\n","import folium\n","from folium.plugins import MarkerCluster\n","from folium import Map, Marker, PolyLine\n","import matplotlib.cm as cm\n","import matplotlib.colors as colors\n","import time\n","from shapely.geometry import MultiPoint\n","from streamlit_folium import folium_static\n","\n","# Naver API ì„¤ì •\n","NAVER_CLIENT_ID = \"lufec25vd7\"\n","NAVER_CLIENT_SECRET = \"9tc3oII8TjHJgrdX9sgdgIEt8ieNeRuaacQG2ADB\"\n","\n","# ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì •\n","image_directory = '/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/filtered_images_new'\n","\n","# í•„ìš”í•œ ë°ì´í„°ì…‹ ëª¨ë‘ ë¡œë“œ\n","final_similarity_df = pd.read_csv(\"/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/final_similarity_df_1112.csv\")\n","with open('/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/aihub_embeddings_1112.json', 'r') as f:\n","    loaded_aihub_embeddings_list = json.load(f)\n","recommend_table = pd.read_csv('/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/recommend_table_1111.csv')\n","spot_table = pd.read_csv('/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/spot_table_1111.csv')\n","act_consume = pd.read_csv('/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/tn_activity_consume_his_á„’á…ªá†¯á„ƒá…©á†¼á„‰á…©á„‡á…µá„‚á…¢á„‹á…§á†¨_H.csv')\n","recommend_with_consume = pd.read_csv(\"/content/drive/MyDrive/contest/K-á„ƒá…¦á„‹á…µá„á…¥á„‰á…¡á„‹á…µá„‹á…¥á†«á„‰á…³ á„’á…¢á„á…¥á„á…©á†«/recommend_with_consume_1111.csv\")\n","\n","# ëª¨ë¸ ì„¤ì •ì„ ìºì‹œí•˜ì—¬ í•œë²ˆë§Œ ë¡œë“œ\n","@st.cache_resource\n","def load_model():\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_compute_dtype=torch.float16\n","    )\n","    processor = AutoProcessor.from_pretrained('llava-hf/llava-1.5-7b-hf')\n","    model1 = LlavaForConditionalGeneration.from_pretrained(\n","        'llava-hf/llava-1.5-7b-hf',\n","        quantization_config=quantization_config,\n","        device_map=\"auto\"\n","    )\n","    model2 = SentenceTransformer('bert-base-nli-mean-tokens')\n","    return processor, model1, model2\n","\n","\n","# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n","def calculate_pairwise_cosine_similarity(user_image, aihub_image_embeddings_list, category):\n","    _, _, model2 = load_model()\n","    sentences_user = user_image[category].tolist()\n","    sentences_user_embedding = model2.encode(sentences_user)  # user ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì„ë² ë”©\n","\n","    similarity_list = []\n","    # user ì´ë¯¸ì§€ì˜ ê° ì„ë² ë”©ê³¼ aihub ì„ë² ë”©ì„ ë¹„êµ\n","    for i in range(len(sentences_user_embedding)):\n","        for aihub_entry in aihub_image_embeddings_list:\n","            if aihub_entry['category'] == category:  # ì¹´í…Œê³ ë¦¬ ì²´í¬\n","                aihub_embedding = aihub_entry['embedding']  # ì„ë² ë”©ì„ ê°€ì ¸ì˜´\n","                cosine_sim = cosine_similarity([sentences_user_embedding[i]], [aihub_embedding])[0][0]\n","\n","                # aihub_entryì—ì„œ ì´ë¯¸ì§€ ì •ë³´ë„ ì°¸ì¡°\n","                similarity_list.append({\n","                    'user_image': user_image['image'].iloc[i],\n","                    'aihub_image': aihub_entry['image'],\n","                    'response_column': category,\n","                    'cosine_similarity': cosine_sim\n","                })\n","    # ê²°ê³¼ë¥¼ ë°˜í™˜\n","    return similarity_list\n","\n","\n","# ì„ íƒëœ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ë„ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n","def generate_similarity_info(selected_images, final_similarity_df, similar_images):\n","    similarity_info = []\n","\n","    # ì„ íƒëœ ì´ë¯¸ì§€ì˜ ê° responseì— ëŒ€í•œ ìœ ì‚¬ë„ ì •ë³´ ì €ì¥\n","    for response, selected_image in selected_images.items():\n","        # ì„ íƒëœ ì´ë¯¸ì§€ì™€ responseê°€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ìœ ì‚¬ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸°\n","        selected_similarities = final_similarity_df.loc[\n","            ((final_similarity_df['user_image'] == selected_image) | (final_similarity_df['aihub_image'] == selected_image)) &\n","            (final_similarity_df['response_column'] == response)\n","        ]\n","\n","        for _, row in selected_similarities.iterrows():\n","            other_image = row['aihub_image'] if row['user_image'] == selected_image else row['user_image']\n","            similarity_info.append({\n","                'response': response,\n","                'selected_image': selected_image,\n","                'other_image': other_image,\n","                'cosine_similarity': row['cosine_similarity']\n","            })\n","\n","    # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì¶”ê°€\n","    for response, df in similar_images.items():\n","        for selected_image in selected_images.values():\n","            # í•´ë‹¹ responseì— ì„ íƒëœ ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ëŠ” ê²½ìš°ì—ë§Œ ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì¶”ê°€\n","            if selected_image in df['aihub_image'].values:\n","                similarity_info.append({\n","                    'response': response,\n","                    'selected_image': selected_image,\n","                    'other_image': selected_image,\n","                    'cosine_similarity': 1.0  # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ëŠ” 1ë¡œ ê°€ì •\n","                })\n","\n","    # ê° other_imageì— ëŒ€í•´ cosine similarity ê°’ì„ ëª¨ì•„ í‰ê·  ê³„ì‚°\n","    average_similarities = {}\n","    # similarity_infoì—ì„œ other_imageë³„ë¡œ cosine similarity ëª¨ìŒ\n","    for info in similarity_info:\n","        other_image = info['other_image']\n","        if other_image not in average_similarities:\n","            average_similarities[other_image] = []\n","        average_similarities[other_image].append(info['cosine_similarity'])\n","\n","    # ê° other_imageì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n","    for image, similarities in average_similarities.items():\n","        average_similarities[image] = sum(similarities) / len(similarities)\n","\n","    return average_similarities\n","\n","\n","# ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n","def get_most_similar_image_path(average_similarities):\n","    if average_similarities:\n","        most_similar_image = max(average_similarities, key=average_similarities.get)\n","        most_similar_image_path = os.path.join(image_directory, most_similar_image)\n","        return most_similar_image_path\n","    else:\n","        return None\n","\n","# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n","def similarity(new_traveler, recommend_table_near):\n","    # ëª…ëª©í˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚°: ê°™ìœ¼ë©´ ê°ê° 0.5, ë‹¤ë¥´ë©´ 0\n","    nominal_similarity = (recommend_table_near[['GENDER', 'TRAVEL_MOTIVE_1']] == new_traveler[[0,3]]).astype(int)\n","\n","    age_grp = pd.to_numeric(recommend_table_near['AGE_GRP'], errors='coerce').fillna(0).astype(int)\n","    new_traveler_age_grp = int(new_traveler[1])  # new_travelerì˜ ë‚˜ì´ ê·¸ë£¹ë„ ì •ìˆ˜ë¡œ ë³€í™˜\n","    age_diff = np.abs(age_grp - new_traveler_age_grp)\n","\n","    travel_style = pd.to_numeric(recommend_table_near['TRAVEL_STYL_1'], errors='coerce').fillna(0).astype(int)\n","    new_traveler_travel_style = int(new_traveler[2])\n","    # ì—¬í–‰ ìŠ¤íƒ€ì¼ ì°¨ì´ ê³„ì‚°\n","    travel_style_diff = np.abs(travel_style - new_traveler_travel_style)\n","\n","    ordinal_similarity = pd.DataFrame({\n","        'AGE_SIM': 1 - (age_diff / age_diff.max()),  # ì •ê·œí™”\n","        'STYLE_SIM': 1 - (travel_style_diff / travel_style_diff.max())  # ì •ê·œí™”\n","    })\n","\n","    # ì „ì²´ ìœ ì‚¬ë„: ëª…ëª©í˜•ê³¼ ìˆœì„œí˜• ìœ ì‚¬ë„ì˜ í‰ê· \n","    similarity = (nominal_similarity.mean(axis=1) + ordinal_similarity.mean(axis=1)) / 2\n","    return similarity\n","\n","\n","# ì¶”ì²œ í•¨ìˆ˜\n","def recommend(spot, new_traveler, metric, recommend_with_consume=None, n=5):\n","    spot_coord = spot[['X_COORD', 'Y_COORD']].values\n","    coords = spot_table[['X_COORD', 'Y_COORD']]\n","\n","    if metric == 'knn':\n","        K = 51\n","        knn = NearestNeighbors(n_neighbors=K)\n","        knn.fit(coords)\n","\n","        spot_coord = spot_coord.reshape(1, -1)\n","        dist, idx = knn.kneighbors(spot_coord)\n","        idx = idx.reshape(-1)\n","\n","        near_place = spot_table.loc[idx]\n","        near_place['Distance'] = dist.reshape(-1)\n","    else:\n","        R = 0.1\n","        spot_coord = spot_coord.reshape(-1)\n","        x, y = spot_coord[0], spot_coord[1]\n","\n","        near_place = spot_table.copy()\n","        near_place['Distance'] = np.sqrt((near_place['X_COORD'] - x)**2 + (near_place['Y_COORD'] - y)**2)\n","        near_place = near_place[near_place.Distance < R]\n","\n","    near_POI = near_place['POI_ID']\n","    recommend_table_near = spot_table[spot_table.POI_ID.isin(near_POI)].reset_index(drop=True)\n","    similar_travelers = recommend_table_near.copy()\n","    similar_travelers['SIMILARITY'] = similarity(new_traveler, recommend_table_near)\n","\n","    max_idx = similar_travelers.groupby('POI_ID')['SIMILARITY'].idxmax()\n","\n","    result = similar_travelers.loc[max_idx]\n","    result = result[result.ratings >= 4]\n","\n","    result = result.sort_values(by='SIMILARITY', ascending=False).iloc[:n]\n","\n","    return result\n","\n","# íŠ¹ì • ì¥ì†Œì™€ ì†Œë¹„ ë‚´ì—­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œë³„ í‰ê·  ê²°ì œ ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\n","def locations_with_time(locations, act_consume):\n","\n","    # ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n","    locations['avg_payment_time'] = None\n","\n","    for idx, row in locations.iterrows():\n","        road_nm_addr = row['ROAD_NM_ADDR']\n","\n","        # act_consume ë°ì´í„°í”„ë ˆì„ì—ì„œ í•´ë‹¹ ROAD_NM_ADDRë¥¼ ê°€ì§€ëŠ” í–‰ í•„í„°ë§\n","        matching_rows = act_consume[act_consume['ROAD_NM_ADDR'] == road_nm_addr]\n","\n","        if not matching_rows.empty:\n","            # PAYMENT_DTì—ì„œ ì‹œê°„ ë¶€ë¶„ (HH:MM)ë§Œ ì¶”ì¶œí•˜ì—¬ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n","            matching_rows['PAYMENT_TIME'] = pd.to_datetime(matching_rows['PAYMENT_DT'], errors='coerce').dt.strftime('%H:%M')\n","\n","            # ìœ íš¨í•œ ì‹œê°„ ê°’ë§Œ ì„ íƒ (NaN ê°’ ì œì™¸)\n","            valid_times = matching_rows['PAYMENT_TIME'].dropna()\n","            if not valid_times.empty:\n","                # ì‹œê°„ ë¶€ë¶„ì„ 'HH:MM' í¬ë§·ì˜ ë¬¸ìì—´ë¡œ ìœ ì§€í•˜ì—¬ hourì™€ minuteì„ ë¶„ë¦¬ í›„ ì´ˆë¡œ ë³€í™˜\n","                valid_times_minutes = valid_times.apply(lambda t: int(t[:2]) * 60 + int(t[3:]))\n","                avg_minutes = valid_times_minutes.mean()\n","\n","                # í‰ê·  ë¶„ì„ 'HH:MM' í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n","                avg_hours = int(avg_minutes // 60)\n","                avg_minutes = int(avg_minutes % 60)\n","\n","                # í‰ê·  ì‹œê°„ì„ 'HH:MM' í˜•ì‹ìœ¼ë¡œ ì €ì¥\n","                avg_payment_time = f\"{avg_hours:02}:{avg_minutes:02}\"\n","                locations.at[idx, 'avg_payment_time'] = avg_payment_time\n","            else:\n","                # ìœ íš¨í•œ ì‹œê°„ì´ ì—†ì„ ê²½ìš° None í• ë‹¹\n","                locations.at[idx, 'avg_payment_time'] = None\n","\n","    return locations\n","\n","\n","def extract_dining(locations, act_consume):\n","    # ê° ì¥ì†Œë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n","    location_results = []\n","\n","    # locationsì˜ ê° í–‰ì— ëŒ€í•´ ë°˜ë³µ\n","    for _, location_row in locations.iterrows():\n","        target_travel_id = location_row['TRAVEL_ID']\n","        target_road_nm_addr = location_row['ROAD_NM_ADDR']\n","        poi_name = location_row['POI_NM']\n","\n","        # act_consumeì—ì„œ í•´ë‹¹ TRAVEL_IDë¥¼ ê°€ì§€ëŠ” ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n","        filtered_df = act_consume[act_consume['TRAVEL_ID'] == target_travel_id]\n","\n","        # PAYMENT_DT ìˆœìœ¼ë¡œ ì •ë ¬\n","        sorted_df = filtered_df.sort_values(by='PAYMENT_DT').reset_index(drop=True)\n","\n","        # ê¸°ì¤€ì´ ë˜ëŠ” í–‰ ì°¾ê¸° (ROAD_NM_ADDRê°€ ë™ì¼í•œ ì²« ë²ˆì§¸ í–‰)\n","        reference_row = sorted_df[sorted_df['ROAD_NM_ADDR'] == target_road_nm_addr].head(1)\n","\n","        if reference_row.empty:\n","            # ê¸°ì¤€ í–‰ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°\n","            location_results.append((poi_name, pd.DataFrame()))\n","            continue\n","\n","        # ê¸°ì¤€ í–‰ì˜ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n","        ref_index = reference_row.index[0]\n","\n","        # ì „í›„ë¡œ ACTIVITY_TYPE_CD ê°’ì´ 1ì¸ ë°ì´í„°ë¥¼ ì°¾ê¸°\n","        activity_data = []\n","\n","        # ì´ì „ í–‰ì—ì„œ ìµœëŒ€ 2ê°œì˜ í–‰ ì°¾ê¸°\n","        count = 0\n","        for i in range(ref_index - 1, -1, -1):\n","            if sorted_df.loc[i, 'ACTIVITY_TYPE_CD'] == 1:\n","                activity_data.append(sorted_df.loc[i])\n","                count += 1\n","                if count == 2:\n","                    break\n","\n","        # ì´í›„ í–‰ì—ì„œ ìµœëŒ€ 2ê°œì˜ í–‰ ì°¾ê¸°\n","        count = 0\n","        for i in range(ref_index + 1, len(sorted_df)):\n","            if sorted_df.loc[i, 'ACTIVITY_TYPE_CD'] == 1:\n","                activity_data.append(sorted_df.loc[i])\n","                count += 1\n","                if count == 2:\n","                    break\n","\n","        # ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— (POI_NM, ë°ì´í„°í”„ë ˆì„) í˜•íƒœë¡œ ì €ì¥\n","        result_df = pd.DataFrame(activity_data)\n","        location_results.append((poi_name, result_df))\n","\n","    return location_results\n","\n","def path_optimizer(spot, locations):\n","\n","    headers = {\n","    \"X-NCP-APIGW-API-KEY-ID\": NAVER_CLIENT_ID,\n","    \"X-NCP-APIGW-API-KEY\": NAVER_CLIENT_SECRET\n","    }\n","\n","    # ì¤‘ë³µëœ ì¢Œí‘œ ì œê±° ë° ì´ˆê¸°í™”\n","    locations = locations.drop_duplicates(subset=['X_COORD', 'Y_COORD'], keep='first')\n","    locations = locations[~locations['VISIT_AREA_NM'].isin(spot['VISIT_AREA_NM'])].reset_index(drop=True)\n","\n","    spot_coord = spot[['X_COORD', 'Y_COORD']]\n","    coordinates = pd.concat([spot_coord, locations[['X_COORD', 'Y_COORD']]], axis=0, ignore_index=True).values\n","    num_locations = len(coordinates)\n","    distance_matrix = np.zeros((num_locations, num_locations))\n","    duration_matrix = np.zeros((num_locations, num_locations))\n","\n","    # ê±°ë¦¬ ë° ì†Œìš” ì‹œê°„ ê³„ì‚°\n","    for i in range(num_locations):\n","        for j in range(num_locations):\n","            if i != j:\n","                start = f\"{coordinates[i][0]},{coordinates[i][1]}\"\n","                end = f\"{coordinates[j][0]},{coordinates[j][1]}\"\n","                url = f\"https://naveropenapi.apigw.ntruss.com/map-direction/v1/driving?start={start}&goal={end}&option=trafast\"\n","                response = requests.get(url, headers=headers)\n","                if response.status_code == 200:\n","                    result = response.json()\n","                    if result['route']:\n","                        distance = result['route']['trafast'][0]['summary']['distance']\n","                        duration = result['route']['trafast'][0]['summary']['duration']\n","                        distance_matrix[i][j] = distance\n","                        duration_matrix[i][j] = duration\n","                    else:\n","                        st.write(f\"ê²½ë¡œ ì—†ìŒ: {i} -> {j}\")\n","                else:\n","                    st.write(f\"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")\n","\n","    # ìµœì  ê²½ë¡œ ì°¾ê¸°\n","    min_path, min_duration, total_distance = None, float('inf'), 0\n","    for perm in itertools.permutations(range(1, num_locations)):\n","        current_path = [0] + list(perm)\n","        current_duration, current_distance = 0, 0\n","        for k in range(len(current_path) - 1):\n","            current_duration += duration_matrix[current_path[k]][current_path[k + 1]]\n","            current_distance += distance_matrix[current_path[k]][current_path[k + 1]]\n","\n","        if current_duration < min_duration:\n","            min_duration = current_duration\n","            total_distance = current_distance\n","            min_path = current_path\n","\n","    spot_df = pd.DataFrame(spot)\n","    ind = min_path[1:]\n","    ind = [x - 1 for x in ind]\n","    result = pd.concat([spot_df, locations.loc[ind]]).reset_index(drop=True)\n","    st.subheader(\"âœ¨ì´ ì´ë™ ì†Œìš” ì‹œê°„\")\n","    st.write(f\"{int(min_duration / 60000)}ë¶„\")\n","    st.subheader(\"âœ¨ì´ ê±°ë¦¬\")\n","    st.write(f\"{total_distance}m\")\n","    #st.write(\"ì—¬ì • ìˆœì„œ ë° ì •ë³´\")\n","    #result2 = result[['POI_NM', 'ratings', 'SIMILARITY']]\n","    #st.dataframe(result2)\n","\n","    # ê° POI ê°„ì˜ ì†Œìš” ì‹œê°„ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ë„ì°©ì§€ ê¸°ì¤€)\n","    travel_times = []\n","    for i in range(len(min_path) - 1):\n","        start_index = min_path[i]\n","        end_index = min_path[i + 1]\n","        travel_time = round(duration_matrix[start_index, end_index] / 60000)  # ms -> min\n","\n","        # ì¶œë°œì§€ì™€ ë„ì°©ì§€ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •\n","        start_location = locations.iloc[start_index - 1]['VISIT_AREA_NM'] if start_index > 0 else spot.iloc[0]['VISIT_AREA_NM']\n","        end_location = locations.iloc[end_index - 1]['VISIT_AREA_NM'] if end_index > 0 else spot.iloc[0]['VISIT_AREA_NM']\n","        travel_times.append({\n","            'ì¶œë°œ': start_location,\n","            'ë„ì°©': end_location,\n","            'ì†Œìš”ì‹œê°„': travel_time\n","        })\n","\n","    # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","    st.subheader(\"âœ¨ì—¬ì • ìˆœì„œ ë° ì •ë³´\")\n","    travel_df = pd.DataFrame(travel_times)\n","    st.table(travel_df)\n","\n","    return result, min_duration, total_distance, duration_matrix, distance_matrix\n","\n","\n","# ê° POI_NMë³„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³€í™˜\n","def convert_dining_to_multiindex(dining):\n","    data = []\n","    for poi_name, dining_df in dining:\n","        dining_df['POI_NM'] = poi_name  # ê° DataFrameì— POI_NM ì»¬ëŸ¼ ì¶”ê°€\n","        data.append(dining_df)\n","\n","    # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨\n","    combined_df = pd.concat(data, ignore_index=True)\n","\n","    # MultiIndex ìƒì„±\n","    combined_df.set_index(['POI_NM', combined_df.groupby('POI_NM').cumcount() + 1], inplace=True)\n","\n","    return combined_df\n","\n","\n","# ê²½ë¡œë¥¼ ì¹´ì¹´ì˜¤ APIë¡œ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜\n","def get_route(origin, destination):\n","    # ì¹´ì¹´ì˜¤ API í‚¤ ì„¤ì •\n","    kakao_headers = {\n","        \"Authorization\": \"KakaoAK fcf14da2e76244a746666e84a90b0900\"\n","    }\n","    url = f\"https://apis-navi.kakaomobility.com/v1/directions?origin={origin}&destination={destination}&waypoints=&priority=RECOMMEND&car_fuel=GASOLINE&car_hipass=false&alternatives=false&road_details=false\"\n","    try:\n","        response = requests.get(url, headers=kakao_headers)\n","        response.raise_for_status()\n","        route_data = response.json()\n","        return route_data\n","    except requests.exceptions.RequestException as e:\n","        print(f\"ê²½ë¡œ API ìš”ì²­ ì‹¤íŒ¨: {e}\")\n","        return None\n","\n","# ë„¤ì´ë²„ APIë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ í•¨ìˆ˜\n","def search_image(visit_area_nm, max_retries=10):\n","    # visit_area_nmì„ URL ì¸ì½”ë”©\n","    encoded_query = urllib.parse.quote(visit_area_nm)  # í•œê¸€ì„ URL ì¸ì½”ë”©\n","    url = f\"https://openapi.naver.com/v1/search/image.json?query={encoded_query}&display=1&start=1&sort=sim\"\n","    for attempt in range(max_retries):\n","        try:\n","            headers = {\n","                \"X-Naver-Client-Id\": \"pbn5UWRRNtptJlgjHgyy\",\n","                \"X-Naver-Client-Secret\": \"onAoSavYbe\"\n","            }\n","            response = requests.get(url, headers=headers)\n","            response.raise_for_status()  # ìƒíƒœ ì½”ë“œ 200ì´ ì•„ë‹ ê²½ìš° ì˜ˆì™¸ ë°œìƒ\n","            result = response.json()\n","            if 'items' in result and result['items']:\n","                image_url = result['items'][0]['link']\n","                return image_url\n","            else:\n","                print(f\"No images found for query: {visit_area_nm}\")\n","                return None\n","        except requests.exceptions.RequestException as e:\n","            st.write(f\"API ìš”ì²­ ì‹¤íŒ¨: {e}\")\n","            if attempt < max_retries - 1:\n","                st.write(f\"ì¬ì‹œë„ ì¤‘... {attempt + 1}/{max_retries}\")\n","                time.sleep(2)  # ì ì‹œ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì¬ì‹œë„\n","            else:\n","                st.write(f\"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\")\n","                return None\n","        except ValueError as e:\n","            st.write(f\"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n","            return None\n","    return None\n","\n","# ì§€ë„ì— ê²½ë¡œë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n","def visualize_path_on_map(route_data, route_map, color):\n","    if route_data and 'routes' in route_data and len(route_data['routes']) > 0:\n","        route_coords = route_data['routes'][0]['sections'][0]['roads']\n","        all_vertexes = []\n","        for road in route_coords:\n","            vertexes = road['vertexes']\n","            for i in range(0, len(vertexes), 2):\n","                all_vertexes.append([vertexes[i + 1], vertexes[i]])\n","        folium.PolyLine(\n","            locations=all_vertexes,\n","            color=color,\n","            weight=5,\n","            opacity=0.7,\n","            popup=\"ê²½ë¡œ\"\n","        ).add_to(route_map)\n","\n","def create_map_with_markers_and_ordered_paths(path, poi_store_mapping):\n","    route_map = folium.Map(location=(path.iloc[0]['Y_COORD'], path.iloc[0]['X_COORD']), zoom_start=12)\n","\n","    # ì „ì²´ ê²½ë¡œ ìƒ‰ìƒì€ blueë¡œ í†µì¼\n","    colormap = cm.get_cmap('rainbow', len(path) - 1)\n","    norm = colors.Normalize(vmin=0, vmax=len(path) - 2)\n","\n","    for i in range(len(path) - 1):\n","        origin = f\"{path.iloc[i]['X_COORD']},{path.iloc[i]['Y_COORD']}\"\n","        destination = f\"{path.iloc[i + 1]['X_COORD']},{path.iloc[i + 1]['Y_COORD']}\"\n","        # ì¹´ì¹´ì˜¤ API ê²½ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n","        route_data = get_route(origin, destination)\n","        route_color = 'blue'  # ì „ì²´ ê²½ë¡œëŠ” blueë¡œ í†µì¼\n","\n","        if route_data:\n","            visualize_path_on_map(route_data, route_map, color=route_color)\n","\n","        # ì´ë¯¸ì§€ ê²€ìƒ‰ ë° ë§ˆì»¤ í‘œì‹œ\n","        poi_name = path.iloc[i]['VISIT_AREA_NM']\n","        image_url = search_image(poi_name)\n","\n","        # 'STORE_NM' ëª©ë¡ ì¶”ì¶œ\n","        store_names = poi_store_mapping.loc[poi_store_mapping['POI_NM'] == poi_name, 'STORE_NM']\n","        store_names_list = store_names.iloc[0] if not store_names.empty else []\n","\n","        # STORE_NM ì •ë³´ë¥¼ HTMLë¡œ ë³€í™˜\n","        stores_html = \"<br>\".join(store_names_list) if store_names_list else \"No stores available\"\n","\n","        # íŒì—… HTML êµ¬ì„±\n","        popup_html = f\"<b>{i+1}. {poi_name}</b><br>\"\n","        if image_url:\n","            popup_html += f\"<img src='{image_url}' width='100'><br>\"\n","        popup_html += f\"ì‹ì‚¬ ì¥ì†Œë¡œ ì´ê³³ì€ ì–´ë–¤ê°€ìš”?:<br>{stores_html}\"\n","\n","        # ë§ˆì»¤ ì¶”ê°€\n","        folium.Marker(\n","            location=(path.iloc[i]['Y_COORD'], path.iloc[i]['X_COORD']),\n","            popup=folium.Popup(popup_html, max_width=200),\n","            icon=folium.Icon(color='blue', icon='info-sign')\n","        ).add_to(route_map)\n","\n","    # ë§ˆì§€ë§‰ ì¥ì†Œì— ëŒ€í•œ ë§ˆì»¤ í‘œì‹œ\n","    last_poi_name = path.iloc[-1]['VISIT_AREA_NM']\n","    last_image_url = search_image(last_poi_name)\n","    last_popup_html = f\"<b>{len(path)}. {last_poi_name}</b><br>\"\n","    if last_image_url:\n","        last_popup_html += f\"<img src='{last_image_url}' width='100'><br>\"\n","\n","    folium.Marker(\n","        location=(path.iloc[-1]['Y_COORD'], path.iloc[-1]['X_COORD']),\n","        popup=folium.Popup(last_popup_html, max_width=200),\n","        icon=folium.Icon(color='blue', icon='info-sign')\n","    ).add_to(route_map)\n","\n","    # íŠ¹ì • ì¥ì†Œ í´ë¦­ ì‹œ ê²½ë¡œ í‘œì‹œ\n","    def on_marker_click(marker, place):\n","        # ì¶œë°œì§€ì™€ ë„ì°©ì§€ë¥¼ êµ¬ë¶„í•˜ì—¬ ê²½ë¡œ ìƒ‰ìƒ ë³€ê²½\n","        start_point = marker.location\n","        for i in range(len(path) - 1):\n","            origin = f\"{path.iloc[i]['X_COORD']},{path.iloc[i]['Y_COORD']}\"\n","            destination = f\"{path.iloc[i + 1]['X_COORD']},{path.iloc[i + 1]['Y_COORD']}\"\n","            route_data = get_route(origin, destination)\n","\n","            # ì¶œë°œì§€ì™€ ë„ì°©ì§€ì¼ ê²½ìš° ìƒ‰ìƒ ë‹¤ë¥´ê²Œ\n","            route_color = 'red' if place == 'start' else 'green'\n","\n","            if route_data:\n","                visualize_path_on_map(route_data, route_map, color=route_color)\n","\n","    folium_static(route_map)\n","\n","    return route_map"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dS9h0O_V-cKQ","outputId":"1ff70188-d952-4dde-9844-57f8bfa8fc3c","executionInfo":{"status":"ok","timestamp":1731388951133,"user_tz":-540,"elapsed":4,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}}},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.py\n"]}]},{"cell_type":"markdown","source":["# app.py"],"metadata":{"id":"SNHBp2WIGFWg"}},{"cell_type":"code","source":["''' ì›ë³¸\n","%%writefile app.py\n","\n","import os\n","import streamlit as st\n","from datetime import datetime\n","from PIL import Image, ImageOps\n","import pandas as pd\n","import numpy as np\n","# config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°\n","from config import (\n","    load_model,\n","    calculate_pairwise_cosine_similarity,\n","    generate_similarity_info,\n","    get_most_similar_image_path,\n","    loaded_aihub_embeddings_list,\n","    image_directory,\n","    recommend_table,\n","    locations_with_time,\n","    extract_dining,\n","    recommend,\n","    path_optimizer,\n","    spot_table,\n","    act_consume,\n","    recommend_with_consume,\n","    convert_dining_to_multiindex,\n","    create_map_with_markers_and_ordered_paths\n",")\n","from streamlit_folium import st_folium\n","\n","# ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)\n","processor, model1, model2 = load_model()\n","\n","\n","# ìœ ì € ì—…ë¡œë“œ ì´ë¯¸ì§€ ì €ì¥ í•¨ìˆ˜\n","def save_uploaded_file(directory, file) :\n","    if not os.path.exists(directory) :\n","        os.makedirs(directory)\n","    with open(os.path.join(directory, file.name), 'wb') as f:\n","        f.write(file.getbuffer())\n","    return os.path.join(directory, file.name)\n","\n","\n","# í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n","def generate_responses(image_path):\n","    prompts = [\n","        \"USER: <image>\\n What other travel destinations or travel activities do you have that go well with this place? Please list it in words. \\nASSISTANT:\",\n","        \"USER: <image>\\n Please express the season that goes best with this picture and why. \\nASSISTANT:\",\n","        \"USER: <image>\\n Tell me about the mood, touch and feelings of this image in detail. Use as many adjectives as possible to express them. Reduce objective descriptions and add a lot of subjective expressions. \\nASSISTANT:\",\n","        \"USER: <image>\\n Describe this image objectively and realistically using the objects in this image. Do not include subjective words such as the mood, touch, atmosphere and feelings. \\nASSISTANT:\"\n","        ]\n","    responses = []\n","    image = Image.open(image_path)\n","    for prompt in prompts:\n","        inputs = processor([image], [prompt], padding=True, return_tensors=\"pt\").to(\"cuda\")\n","        output = model1.generate(**inputs, max_new_tokens=100)\n","        generated_text = processor.batch_decode(output, skip_special_tokens=True)\n","        response = generated_text[0].split(\"ASSISTANT:\")[-1].strip() if \"ASSISTANT:\" in generated_text[0] else generated_text[0].strip()\n","        responses.append(response)\n","    return responses\n","\n","# ë©”ì¸ í•¨ìˆ˜\n","def main():\n","    st.title('ì‚¬ì§„ ì† ì¶”ì–µê³¼ ë‹¤ì‹œ ë§Œë‚˜ëŠ” ì—¬í–‰âœ¨')  # ì›¹ì‚¬ì´íŠ¸ ë©”ì¸ ì œëª©\n","    file = st.file_uploader('ì—¬í–‰ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.', type=['jpg','jpeg','png'])\n","\n","    if file is not None :\n","        current_time = datetime.now().isoformat().replace(':', '_')\n","        file.name = current_time + '.jpg'\n","        image_path = save_uploaded_file('tmp', file)  # ì½”ë© ì¢Œì¸¡ ì—´ë©´ tmpë¼ëŠ” í´ë”ê°€ ìƒê¸°ê³  ê±°ê¸°ì— ì‚¬ì§„ ì €ì¥ë¨\n","\n","        # ì„ì‹œ ê³µê°„ ìƒì„± ë° ë©”ì‹œì§€ í‘œì‹œ\n","        message_placeholder = st.empty()\n","        message_placeholder.write(\"ğŸ•µï¸ì‚¬ì§„ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì´ì—ìš”!\")\n","\n","        image = Image.open(file)\n","        image = ImageOps.exif_transpose(image)  # EXIF ì •ë³´ë¥¼ ì´ìš©í•´ ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ íšŒì „\n","        st.image(image.resize((200, 250)))\n","\n","        responses = generate_responses(image_path)\n","        categories = ['response_1', 'response_2', 'response_3', 'response_4']\n","        category_names = {\n","                'response_1': 'ë¹„ìŠ·í•œ ì—¬í–‰ì§€',\n","                'response_2': 'ê³„ì ˆ',\n","                'response_3': 'ê°ì •',\n","                'response_4': 'ê°ê´€ì  ë¬˜ì‚¬'\n","            }\n","        df_responses = pd.DataFrame([responses], columns=categories)\n","        df_responses['image'] = file.name\n","\n","        message_placeholder.write(\"ğŸ”ë¹„ìŠ·í•œ ì‚¬ì§„ì„ ì°¾ê³  ìˆì–´ìš”...\")\n","\n","        all_similarity_results = [\n","                result\n","                for category in categories\n","                for result in calculate_pairwise_cosine_similarity(df_responses, loaded_aihub_embeddings_list, category)\n","            ]\n","        final_similarity_df_test = pd.DataFrame(all_similarity_results)\n","        similar_images = {\n","                category: final_similarity_df_test[final_similarity_df_test['response_column'] == category]\n","                .nlargest(3, 'cosine_similarity')\n","                .reset_index(drop=True)\n","                for category in categories\n","            }\n","\n","        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n","        for key, default in {\n","            \"selected_images\": {category: None for category in categories},\n","            \"form_submitted\": False,\n","            \"profile_submitted\": False,\n","            \"new_traveler\": None,\n","            \"num_recommendations\": 5,\n","            \"user_profile\": {}\n","        }.items():\n","            if key not in st.session_state:\n","                st.session_state[key] = default\n","\n","        message_placeholder.write(\"ğŸ’–ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\")\n","\n","        # ì´ë¯¸ì§€ ì„ íƒ í¼\n","        if not st.session_state['form_submitted']:\n","            with st.form(\"selection_form\"):\n","                for category in categories:\n","                    display_name = category_names.get(category, category)\n","                    st.write(f\"âœ…{display_name}\")\n","                    cols = st.columns(3)\n","                    options = []\n","                    for i, (col, (_, row)) in enumerate(zip(cols, similar_images[category].iterrows())):\n","                        filename = row['aihub_image']\n","                        image_path = os.path.join(image_directory, filename)\n","                        with col:\n","                            if os.path.exists(image_path):\n","                                img = Image.open(image_path)\n","                                img = ImageOps.exif_transpose(img)\n","                                img = img.resize((200, 200))\n","                                st.image(img, caption=f\"{i + 1}\", use_container_width=True)\n","                            else:\n","                                st.write(f\"{i + 1}. ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}\")\n","                            options.append((f\"{i + 1}\", filename))\n","                    # ë¼ë””ì˜¤ ë²„íŠ¼ì„ í†µí•´ ì´ë¯¸ì§€ ì„ íƒ, ê° ë¼ë””ì˜¤ì˜ í‚¤ëŠ” ì¹´í…Œê³ ë¦¬ì™€ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©í•´ ìœ ì¼í•˜ê²Œ ì„¤ì •\n","                    selected_option = st.radio(\n","                        \"ğŸ’–ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\",\n","                        [opt[0] for opt in options],\n","                        key = f\"{category}_radio_{i}\",\n","                        horizontal = True\n","                    )\n","                    # ì„ íƒëœ íŒŒì¼ëª…ì„ session_stateì— ì €ì¥\n","                    try:\n","                        st.session_state.selected_images[category] = next(\n","                            filename for label, filename in options if label == selected_option\n","                        )\n","                    except StopIteration:\n","                        st.write(\"ì„ íƒëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","                if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                    st.session_state.form_submitted = True\n","\n","        message_placeholder.write(\"ğŸ‰ê°€ì¥ ë¹„ìŠ·í•œ ì¶”ì–µì„ ì°¾ì•˜ì–´ìš”!\")\n","        # í¼ ì œì¶œ í›„ ìµœì¢… ìœ ì‚¬ ì´ë¯¸ì§€ ê³„ì‚°\n","        if st.session_state[\"form_submitted\"] and not st.session_state[\"profile_submitted\"]:\n","            # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± ë° í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n","            average_similarities = generate_similarity_info(\n","                    st.session_state.selected_images,\n","                    final_similarity_df_test,\n","                    similar_images\n","                )\n","            most_similar_image_path = get_most_similar_image_path(average_similarities)\n","            del st.session_state['form_submitted']\n","            if most_similar_image_path and os.path.exists(most_similar_image_path):\n","                most_similar_image = os.path.basename(most_similar_image_path)\n","                image = Image.open(most_similar_image_path)\n","                image = ImageOps.exif_transpose(image)\n","                image = image.resize((200, 250))\n","                st.image(image)\n","                spot = recommend_table.loc[recommend_table['PHOTO_FILE_NM'] == most_similar_image]\n","                if spot.empty:\n","                    st.write('ì¶”ì²œ ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”.')\n","                else:\n","                    st.table(spot)\n","\n","                    # ì‚¬ìš©ì íŠ¹ì„± ì…ë ¥ í¼\n","                    travel_style_mapping = {\n","                        'ìì—° ë§¤ìš° ì„ í˜¸': 1,\n","                        'ìì—° ì¤‘ê°„ ì„ í˜¸': 2,\n","                        'ìì—° ì•½ê°„ ì„ í˜¸': 3,\n","                        'ì¤‘ë¦½': 4,\n","                        'ë„ì‹œ ì•½ê°„ ì„ í˜¸': 5,\n","                        'ë„ì‹œ ì¤‘ê°„ ì„ í˜¸': 6,\n","                        'ë„ì‹œ ë§¤ìš° ì„ í˜¸': 7\n","                    }\n","                    travel_motive_mapping = {\n","                        'ì¼ìƒì ì¸ í™˜ê²½ ë° ì—­í• ì—ì„œì˜ íƒˆì¶œ, ì§€ë£¨í•¨ íƒˆí”¼': 1,\n","                        'ì‰´ ìˆ˜ ìˆëŠ” ê¸°íšŒ, ìœ¡ì²´ í”¼ë¡œ í•´ê²° ë° ì •ì‹ ì ì¸ íœ´ì‹': 2,\n","                        'ì—¬í–‰ ë™ë°˜ìì™€ì˜ ì¹œë°€ê° ë° ìœ ëŒ€ê° ì¦ì§„': 3,\n","                        'ì§„ì •í•œ ìì•„ ì°¾ê¸° ë˜ëŠ” ìì‹ ì„ ë˜ëŒì•„ë³¼ ê¸°íšŒ ì°¾ê¸°': 4,\n","                        'SNS ì‚¬ì§„ ë“±ë¡ ë“± ê³¼ì‹œ': 5,\n","                        'ìš´ë™, ê±´ê°• ì¦ì§„ ë° ì¶©ì „': 6,\n","                        'ìƒˆë¡œìš´ ê²½í—˜ ì¶”êµ¬': 7,\n","                        'ì—­ì‚¬ íƒë°©, ë¬¸í™”ì  ê²½í—˜ ë“± êµìœ¡ì  ë™ê¸°': 8,\n","                        'íŠ¹ë³„í•œ ëª©ì (ì¹ ìˆœì—¬í–‰, ì‹ í˜¼ì—¬í–‰, ìˆ˜í•™ì—¬í–‰, ì¸ì„¼í‹°ë¸Œì—¬í–‰)': 9,\n","                        'ê¸°íƒ€': 10\n","                    }\n","                    st.subheader(\"ë³¸ì¸ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\")\n","                    with st.form(\"user_profile_form\"):\n","                        gender = st.radio(\"ì„±ë³„\", [0, 1], format_func=lambda x: \"ë‚¨ì„±\" if x == 0 else \"ì—¬ì„±\",\n","                                            horizontal=True)\n","                        age_group_label = st.radio(\"ë‚˜ì´\",\n","                        ['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€'], horizontal=True)\n","                        age_group = int(age_group_label[:-1])\n","                        travel_style_label = st.radio(\"ìì—° vs ë„ì‹œ\", list(travel_style_mapping.keys()))\n","                        travel_style = travel_style_mapping[travel_style_label]\n","                        travel_motive_label = st.radio(\"ì—¬í–‰ ë™ê¸°\", list(travel_motive_mapping.keys()))\n","                        travel_motive = travel_motive_mapping[travel_motive_label]\n","                        num_recommendations = st.slider(\"ì¶”ì²œë°›ê³  ì‹¶ì€ ì¥ì†Œ ê°œìˆ˜\", min_value=3, max_value=10, value=5, step=1)\n","\n","                        if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                            st.session_state.user_profile = {\n","                                    \"GENDER\": gender,\n","                                    \"AGE_GRP\": age_group,\n","                                    \"TRAVEL_STYL_1\": travel_style,\n","                                    \"TRAVEL_MOTIVE_1\": travel_motive,\n","                                    \"NUM_RECOMMENDATIONS\": num_recommendations\n","                                }\n","                            st.session_state.profile_submitted = True\n","                            st.write(\"ì‚¬ìš©ì í”„ë¡œí•„ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","                            st.session_state.new_traveler = np.array([gender, age_group, travel_style, travel_motive])\n","                            st.session_state.num_recommendations = num_recommendations\n","        st.session_state.profile_submitted = True\n","        # ì¶”ì²œ ì¥ì†Œ ë° ì¶”ê°€ ì •ë³´ ì¶”ì¶œ\n","        if st.session_state.profile_submitted:\n","            locations = recommend(\n","                            spot,\n","                            st.session_state.new_traveler,\n","                            metric = 'knn',\n","                            n = st.session_state.num_recommendations,\n","                            recommend_with_consume = recommend_with_consume\n","                        )\n","            locations_time = locations_with_time(locations, act_consume)\n","            locations_time_print = locations_time[['VISIT_AREA_NM',\n","                                                   'ROAD_NM_ADDR',\n","                                                   'ratings']]\n","            locations_time_print.columns = ['ì´ë¦„', 'ì£¼ì†Œ', 'ë³„ì ']\n","\n","            st.subheader(\"ì¶”ì²œ ì¥ì†Œ\")\n","            st.table(locations_time_print)\n","\n","            dining = extract_dining(locations_time, act_consume)\n","\n","\n","            # ê²½ë¡œ ìµœì í™” ìˆ˜í–‰\n","            path, duration, dist, duration_matrix, _ = path_optimizer(spot, locations)\n","\n","            # diningì„ MultiIndex DataFrameìœ¼ë¡œ ë³€í™˜\n","            dining_multiindex_df = convert_dining_to_multiindex(dining)\n","            poi_store_mapping = dining_multiindex_df.groupby('POI_NM')['STORE_NM'].apply(list).reset_index()\n","            st.table(poi_store_mapping)\n","\n","            # ê²½ë¡œì™€ ë§ˆì»¤ê°€ í¬í•¨ëœ ì§€ë„ ìƒì„±\n","            route_map = create_map_with_markers_and_ordered_paths(path, poi_store_mapping)\n","\n","if __name__=='__main__' :\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7bb12e4c-cd37-4ebd-94af-5e111ab93a25","id":"j2SfVfiO-61B","executionInfo":{"status":"ok","timestamp":1731383679090,"user_tz":-540,"elapsed":611,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["## ì›ë³¸"],"metadata":{"id":"tnfYJINN0Rr_"}},{"cell_type":"code","source":["''' ì›ë³¸\n","# íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ëŠ” ë°©ë²•\n","%%writefile app.py\n","\n","import os\n","import streamlit as st\n","from datetime import datetime\n","from PIL import Image, ImageOps\n","import pandas as pd\n","import numpy as np\n","# config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°\n","from config import (\n","    load_model,\n","    calculate_pairwise_cosine_similarity,\n","    generate_similarity_info,\n","    get_most_similar_image_path,\n","    loaded_aihub_embeddings_list,\n","    image_directory,\n","    recommend_table,\n","    locations_with_time,\n","    extract_dining,\n","    recommend,\n","    path_optimizer,\n","    spot_table,\n","    act_consume,\n","    recommend_with_consume,\n","    convert_dining_to_multiindex,\n","    create_map_with_markers_and_ordered_paths\n",")\n","from streamlit_folium import st_folium\n","\n","# ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)\n","processor, model1, model2 = load_model()\n","\n","\n","# ìœ ì € ì—…ë¡œë“œ ì´ë¯¸ì§€ ì €ì¥ í•¨ìˆ˜\n","def save_uploaded_file(directory, file) :\n","    if not os.path.exists(directory) :\n","        os.makedirs(directory)\n","    with open(os.path.join(directory, file.name), 'wb') as f:\n","        f.write(file.getbuffer())\n","    return os.path.join(directory, file.name)\n","\n","\n","# í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n","def generate_responses(image_path):\n","    prompts = [\n","        \"USER: <image>\\n What other travel destinations or travel activities do you have that go well with this place? Please list it in words. \\nASSISTANT:\",\n","        \"USER: <image>\\n Please express the season that goes best with this picture and why. \\nASSISTANT:\",\n","        \"USER: <image>\\n Tell me about the mood, touch and feelings of this image in detail. Use as many adjectives as possible to express them. Reduce objective descriptions and add a lot of subjective expressions. \\nASSISTANT:\",\n","        \"USER: <image>\\n Describe this image objectively and realistically using the objects in this image. Do not include subjective words such as the mood, touch, atmosphere and feelings. \\nASSISTANT:\"\n","        ]\n","    responses = []\n","    image = Image.open(image_path)\n","\n","    for prompt in prompts:\n","        inputs = processor([image], [prompt], padding=True, return_tensors=\"pt\").to(\"cuda\")\n","        output = model1.generate(**inputs, max_new_tokens=100)\n","        generated_text = processor.batch_decode(output, skip_special_tokens=True)\n","        response = generated_text[0].split(\"ASSISTANT:\")[-1].strip() if \"ASSISTANT:\" in generated_text[0] else generated_text[0].strip()\n","        responses.append(response)\n","\n","    return responses\n","\n","# ë©”ì¸ í•¨ìˆ˜\n","def main():\n","    st.title('ì‚¬ì§„ ì† ì¶”ì–µê³¼ ë‹¤ì‹œ ë§Œë‚˜ëŠ” ì—¬í–‰')  # ì›¹ì‚¬ì´íŠ¸ ë©”ì¸ ì œëª©\n","    file = st.file_uploader('ì—¬í–‰ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.', type=['jpg','jpeg','png'])\n","\n","    if file is not None :\n","        current_time = datetime.now().isoformat().replace(':', '_')\n","        file.name = current_time + '.jpg'\n","        image_path = save_uploaded_file('tmp', file)  # ì½”ë© ì¢Œì¸¡ ì—´ë©´ tmpë¼ëŠ” í´ë”ê°€ ìƒê¸°ê³  ê±°ê¸°ì— ì‚¬ì§„ ì €ì¥ë¨\n","\n","        # ì„ì‹œ ê³µê°„ ìƒì„± ë° ë©”ì‹œì§€ í‘œì‹œ\n","        message_placeholder = st.empty()\n","\n","        image = Image.open(file)\n","        image = ImageOps.exif_transpose(image)  # EXIF ì •ë³´ë¥¼ ì´ìš©í•´ ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ íšŒì „\n","        st.image(image.resize((250, 300)))\n","\n","\n","        message_placeholder.write(\"ì˜¬ë ¤ì£¼ì‹  ì‚¬ì§„ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì´ì—ìš”!\")\n","\n","        responses = generate_responses(image_path)\n","        categories = ['response_1', 'response_2', 'response_3', 'response_4']\n","        category_names = {\n","                'response_1': 'ë¹„ìŠ·í•œ ì—¬í–‰ì§€',\n","                'response_2': 'ê³„ì ˆ',\n","                'response_3': 'ê°ì •',\n","                'response_4': 'ê°ê´€ì  ë¬˜ì‚¬'\n","            }\n","        df_responses = pd.DataFrame([responses], columns=categories)\n","        df_responses['image'] = file.name\n","\n","        message_placeholder.write(\"ë¹„ìŠ·í•œ ì‚¬ì§„ì„ ì°¾ê³  ìˆì–´ìš”...\")\n","\n","        all_similarity_results = [\n","                result\n","                for category in categories\n","                for result in calculate_pairwise_cosine_similarity(df_responses, loaded_aihub_embeddings_list, category)\n","            ]\n","\n","        final_similarity_df_test = pd.DataFrame(all_similarity_results)\n","\n","        similar_images = {\n","                category: final_similarity_df_test[final_similarity_df_test['response_column'] == category]\n","                .nlargest(3, 'cosine_similarity')\n","                .reset_index(drop=True)\n","                for category in categories\n","            }\n","\n","        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n","        for key, default in {\n","            \"selected_images\": {category: None for category in categories},\n","            \"form_submitted\": False,\n","            \"profile_submitted\": False,\n","            \"new_traveler\": None,\n","            \"num_recommendations\": 5,\n","            \"user_profile\": {}\n","        }.items():\n","            if key not in st.session_state:\n","                st.session_state[key] = default\n","\n","        message_placeholder.write(\"ë¹„ìŠ·í•œ ì‚¬ì§„ì„ ì°¾ì•˜ì–´ìš”! ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\")\n","\n","        # ì´ë¯¸ì§€ ì„ íƒ í¼\n","        if not st.session_state['form_submitted']:\n","            with st.form(\"selection_form\"):\n","                for category in categories:\n","                    display_name = category_names.get(category, category)\n","                    st.write(f\"{display_name}ì— ëŒ€í•œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.\")\n","                    cols = st.columns(3)\n","                    options = []\n","                    for i, (col, (_, row)) in enumerate(zip(cols, similar_images[category].iterrows())):\n","                        filename = row['aihub_image']\n","                        image_path = os.path.join(image_directory, filename)\n","                        with col:\n","                            if os.path.exists(image_path):\n","                                img = Image.open(image_path).resize((200, 200))\n","                                st.image(img, caption=f\"{i + 1}\", use_container_width=True)\n","                            else:\n","                                st.write(f\"{i + 1}. ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}\")\n","                            options.append((f\"{i + 1}\", filename))\n","                    # ë¼ë””ì˜¤ ë²„íŠ¼ì„ í†µí•´ ì´ë¯¸ì§€ ì„ íƒ, ê° ë¼ë””ì˜¤ì˜ í‚¤ëŠ” ì¹´í…Œê³ ë¦¬ì™€ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©í•´ ìœ ì¼í•˜ê²Œ ì„¤ì •\n","                    selected_option = st.radio(\n","                        \"ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\",\n","                        [opt[0] for opt in options],\n","                        key = f\"{category}_radio_{i}\",\n","                        horizontal = True\n","                    )\n","                    # ì„ íƒëœ íŒŒì¼ëª…ì„ session_stateì— ì €ì¥\n","                    try:\n","                        st.session_state.selected_images[category] = next(\n","                            filename for label, filename in options if label == selected_option\n","                        )\n","                    except StopIteration:\n","                        st.write(\"ì„ íƒëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","                if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                    st.session_state.form_submitted = True\n","        message_placeholder.write(\"ëª¨ë“  ì„ íƒì„ ì™„ë£Œí–ˆì–´ìš”! ì´ì œ ìµœì¢… ì‚¬ì§„ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.\")\n","        # í¼ ì œì¶œ í›„ ìµœì¢… ìœ ì‚¬ ì´ë¯¸ì§€ ê³„ì‚°\n","        if st.session_state[\"form_submitted\"] and not st.session_state[\"profile_submitted\"]:\n","            # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± ë° í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n","            average_similarities = generate_similarity_info(\n","                    st.session_state.selected_images,\n","                    final_similarity_df_test,\n","                    similar_images\n","                )\n","            # ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n","            most_similar_image_path = get_most_similar_image_path(average_similarities)\n","            if most_similar_image_path and os.path.exists(most_similar_image_path):\n","                message_placeholder.write(\"ê°€ì¥ ë¹„ìŠ·í•œ ì¶”ì–µì„ ì°¾ì•˜ì–´ìš”!\")\n","                most_similar_image = os.path.basename(most_similar_image_path)\n","                image = Image.open(most_similar_image_path).resize((250, 300))\n","                st.image(image)\n","                # ì¡°ê±´ì— ë§ëŠ” recommend_table í–‰ ì¶”ì¶œ\n","                spot = recommend_table.loc[recommend_table['PHOTO_FILE_NM'] == most_similar_image]\n","                st.write(\"ìµœì¢… ì‚¬ì§„ ì—¬í–‰ì§€ ì •ë³´:\", spot)\n","\n","                # ì‚¬ìš©ì íŠ¹ì„± ì…ë ¥ í¼\n","                travel_style_mapping = {\n","                    'ìì—° ë§¤ìš° ì„ í˜¸': 1,\n","                    'ìì—° ì¤‘ê°„ ì„ í˜¸': 2,\n","                    'ìì—° ì•½ê°„ ì„ í˜¸': 3,\n","                    'ì¤‘ë¦½': 4,\n","                    'ë„ì‹œ ì•½ê°„ ì„ í˜¸': 5,\n","                    'ë„ì‹œ ì¤‘ê°„ ì„ í˜¸': 6,\n","                    'ë„ì‹œ ë§¤ìš° ì„ í˜¸': 7\n","                }\n","\n","                travel_motive_mapping = {\n","                    'ì¼ìƒì ì¸ í™˜ê²½ ë° ì—­í• ì—ì„œì˜ íƒˆì¶œ, ì§€ë£¨í•¨ íƒˆí”¼': 1,\n","                    'ì‰´ ìˆ˜ ìˆëŠ” ê¸°íšŒ, ìœ¡ì²´ í”¼ë¡œ í•´ê²° ë° ì •ì‹ ì ì¸ íœ´ì‹': 2,\n","                    'ì—¬í–‰ ë™ë°˜ìì™€ì˜ ì¹œë°€ê° ë° ìœ ëŒ€ê° ì¦ì§„': 3,\n","                    'ì§„ì •í•œ ìì•„ ì°¾ê¸° ë˜ëŠ” ìì‹ ì„ ë˜ëŒì•„ë³¼ ê¸°íšŒ ì°¾ê¸°': 4,\n","                    'SNS ì‚¬ì§„ ë“±ë¡ ë“± ê³¼ì‹œ': 5,\n","                    'ìš´ë™, ê±´ê°• ì¦ì§„ ë° ì¶©ì „': 6,\n","                    'ìƒˆë¡œìš´ ê²½í—˜ ì¶”êµ¬': 7,\n","                    'ì—­ì‚¬ íƒë°©, ë¬¸í™”ì  ê²½í—˜ ë“± êµìœ¡ì  ë™ê¸°': 8,\n","                    'íŠ¹ë³„í•œ ëª©ì (ì¹ ìˆœì—¬í–‰, ì‹ í˜¼ì—¬í–‰, ìˆ˜í•™ì—¬í–‰, ì¸ì„¼í‹°ë¸Œì—¬í–‰)': 9,\n","                    'ê¸°íƒ€': 10\n","                }\n","\n","                st.subheader(\"ë³¸ì¸ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”\")\n","                with st.form(\"user_profile_form\"):\n","                    gender = st.radio(\"ì„±ë³„\", [0, 1], format_func=lambda x: \"ë‚¨ì„±\" if x == 0 else \"ì—¬ì„±\",\n","                                          horizontal=True)\n","                    age_group_label = st.radio(\"ë‚˜ì´\",\n","                     ['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€'], horizontal=True)\n","                    age_group = int(age_group_label[:-1])\n","                    travel_style_label = st.radio(\"ìì—° vs ë„ì‹œ\", list(travel_style_mapping.keys()), horizontal=True)\n","                    travel_style = travel_style_mapping[travel_style_label]\n","                    travel_motive_label = st.radio(\"ì—¬í–‰ ë™ê¸°\", list(travel_motive_mapping.keys()), horizontal=True)\n","                    travel_motive = travel_motive_mapping[travel_motive_label]\n","                    num_recommendations = st.slider(\"ì¶”ì²œë°›ê³  ì‹¶ì€ ì¥ì†Œ ê°œìˆ˜\", min_value=3, max_value=10, value=5, step=1)\n","\n","                    if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                        st.session_state.user_profile = {\n","                                \"GENDER\": gender,\n","                                \"AGE_GRP\": age_group,\n","                                \"TRAVEL_STYL_1\": travel_style,\n","                                \"TRAVEL_MOTIVE_1\": travel_motive,\n","                                \"NUM_RECOMMENDATIONS\": num_recommendations\n","                            }\n","                        st.session_state.profile_submitted = True\n","                        st.write(\"ì‚¬ìš©ì í”„ë¡œí•„ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","                        st.session_state.new_traveler = np.array([gender, age_group, travel_style, travel_motive])\n","                        st.session_state.num_recommendations = num_recommendations\n","\n","        # ì¶”ì²œ ì¥ì†Œ ë° ì¶”ê°€ ì •ë³´ ì¶”ì¶œ\n","        if st.session_state.profile_submitted:\n","            locations = recommend(\n","                        spot,\n","                        st.session_state.new_traveler,\n","                        metric = 'knn',\n","                        n = st.session_state.num_recommendations,\n","                        recommend_with_consume = recommend_with_consume\n","                    )\n","            locations_time = locations_with_time(locations, act_consume)\n","            dining = extract_dining(locations_time, act_consume)\n","            st.write(\"ì¶”ì²œ ì¥ì†Œ ì •ë³´:\", locations_time)\n","\n","            # ê²½ë¡œ ìµœì í™” ìˆ˜í–‰\n","            path, duration, dist, duration_matrix, _ = path_optimizer(spot, locations)\n","\n","            # diningì„ MultiIndex DataFrameìœ¼ë¡œ ë³€í™˜\n","            dining_multiindex_df = convert_dining_to_multiindex(dining)\n","            poi_store_mapping = dining_multiindex_df.groupby('POI_NM')['STORE_NM'].apply(list).reset_index()\n","            st.dataframe(poi_store_mapping)\n","\n","            # ê²½ë¡œì™€ ë§ˆì»¤ê°€ í¬í•¨ëœ ì§€ë„ ìƒì„±\n","            route_map = create_map_with_markers_and_ordered_paths(path, poi_store_mapping)\n","            # Streamlitì— Folium ì§€ë„ í‘œì‹œ\n","            st.write(\"ìµœì  ê²½ë¡œ ì§€ë„\")\n","            st_folium(route_map, width=700, height=500)\n","\n","if __name__=='__main__' :\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"74d2655d-8d59-409f-8b9a-cf5e3320490e","executionInfo":{"status":"ok","timestamp":1731347269948,"user_tz":-540,"elapsed":4,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}},"id":"lERLKFwFpafP"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["## ì´ˆê¸°ë¡œ ëŒì•„ê°€ì"],"metadata":{"id":"KlC2GJY6iJbt"}},{"cell_type":"code","source":["%%writefile app.py\n","\n","import os\n","import streamlit as st\n","from datetime import datetime\n","from PIL import Image, ImageOps\n","import pandas as pd\n","import numpy as np\n","# config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°\n","from config import (\n","    load_model,\n","    calculate_pairwise_cosine_similarity,\n","    generate_similarity_info,\n","    get_most_similar_image_path,\n","    loaded_aihub_embeddings_list,\n","    image_directory,\n","    recommend_table,\n","    locations_with_time,\n","    extract_dining,\n","    recommend,\n","    path_optimizer,\n","    spot_table,\n","    act_consume,\n","    recommend_with_consume,\n","    convert_dining_to_multiindex,\n","    create_map_with_markers_and_ordered_paths\n",")\n","from streamlit_folium import st_folium\n","\n","# ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)\n","processor, model1, model2 = load_model()\n","\n","\n","# ìœ ì € ì—…ë¡œë“œ ì´ë¯¸ì§€ ì €ì¥ í•¨ìˆ˜\n","def save_uploaded_file(directory, file) :\n","    if not os.path.exists(directory) :\n","        os.makedirs(directory)\n","    with open(os.path.join(directory, file.name), 'wb') as f:\n","        f.write(file.getbuffer())\n","    return os.path.join(directory, file.name)\n","\n","\n","# í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n","def generate_responses(image_path):\n","    prompts = [\n","        \"USER: <image>\\n What other travel destinations or travel activities do you have that go well with this place? Please list it in words. \\nASSISTANT:\",\n","        \"USER: <image>\\n Please express the season that goes best with this picture and why. \\nASSISTANT:\",\n","        \"USER: <image>\\n Tell me about the mood, touch and feelings of this image in detail. Use as many adjectives as possible to express them. Reduce objective descriptions and add a lot of subjective expressions. \\nASSISTANT:\",\n","        \"USER: <image>\\n Describe this image objectively and realistically using the objects in this image. Do not include subjective words such as the mood, touch, atmosphere and feelings. \\nASSISTANT:\"\n","        ]\n","    responses = []\n","    image = Image.open(image_path)\n","\n","    for prompt in prompts:\n","        inputs = processor([image], [prompt], padding=True, return_tensors=\"pt\").to(\"cuda\")\n","        output = model1.generate(**inputs, max_new_tokens=100)\n","        generated_text = processor.batch_decode(output, skip_special_tokens=True)\n","        response = generated_text[0].split(\"ASSISTANT:\")[-1].strip() if \"ASSISTANT:\" in generated_text[0] else generated_text[0].strip()\n","        responses.append(response)\n","\n","    return responses\n","\n","# ë©”ì¸ í•¨ìˆ˜\n","def main():\n","    st.title('ì‚¬ì§„ ì† ì¶”ì–µê³¼ ë‹¤ì‹œ ë§Œë‚˜ëŠ” ì—¬í–‰âœ¨')  # ì›¹ì‚¬ì´íŠ¸ ë©”ì¸ ì œëª©\n","    file = st.file_uploader('ì—¬í–‰ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.', type=['jpg','jpeg','png'])\n","\n","    if file is not None :\n","        current_time = datetime.now().isoformat().replace(':', '_')\n","        file.name = current_time + '.jpg'\n","        image_path = save_uploaded_file('tmp', file)  # ì½”ë© ì¢Œì¸¡ ì—´ë©´ tmpë¼ëŠ” í´ë”ê°€ ìƒê¸°ê³  ê±°ê¸°ì— ì‚¬ì§„ ì €ì¥ë¨\n","\n","        # ì„ì‹œ ê³µê°„ ìƒì„± ë° ë©”ì‹œì§€ í‘œì‹œ\n","        message_placeholder = st.empty()\n","\n","        image = Image.open(file)\n","        image = ImageOps.exif_transpose(image)  # EXIF ì •ë³´ë¥¼ ì´ìš©í•´ ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ íšŒì „\n","        st.image(image.resize((200, 220)))\n","\n","\n","        message_placeholder.write(\"ğŸ•µï¸ì‚¬ì§„ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì´ì—ìš”!\")\n","\n","        responses = generate_responses(image_path)\n","        categories = ['response_1', 'response_2', 'response_3', 'response_4']\n","        category_names = {\n","                'response_1': 'ë¹„ìŠ·í•œ ì—¬í–‰ì§€',\n","                'response_2': 'ê³„ì ˆ',\n","                'response_3': 'ê°ì •',\n","                'response_4': 'ê°ê´€ì  ë¬˜ì‚¬'\n","            }\n","        df_responses = pd.DataFrame([responses], columns=categories)\n","        df_responses['image'] = file.name\n","\n","        message_placeholder.write(\"ğŸ”ë¹„ìŠ·í•œ ì‚¬ì§„ì„ ì°¾ê³  ìˆì–´ìš”...\")\n","\n","        all_similarity_results = [\n","                result\n","                for category in categories\n","                for result in calculate_pairwise_cosine_similarity(df_responses, loaded_aihub_embeddings_list, category)\n","            ]\n","\n","        final_similarity_df_test = pd.DataFrame(all_similarity_results)\n","\n","        similar_images = {\n","                category: final_similarity_df_test[final_similarity_df_test['response_column'] == category]\n","                .nlargest(3, 'cosine_similarity')\n","                .reset_index(drop=True)\n","                for category in categories\n","            }\n","\n","        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n","        for key, default in {\n","            \"selected_images\": {category: None for category in categories},\n","            \"form_submitted\": False,\n","            \"profile_submitted\": False,\n","            \"new_traveler\": None,\n","            \"num_recommendations\": 5,\n","            \"user_profile\": {}\n","        }.items():\n","            if key not in st.session_state:\n","                st.session_state[key] = default\n","\n","        message_placeholder.write(\"ğŸ’–ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\")\n","\n","        # ì´ë¯¸ì§€ ì„ íƒ í¼\n","        if not st.session_state['form_submitted']:\n","            with st.form(\"selection_form\"):\n","                for category in categories:\n","                    display_name = category_names.get(category, category)\n","                    st.write(f\"âœ…{display_name}\")\n","                    cols = st.columns(3)\n","                    options = []\n","                    for i, (col, (_, row)) in enumerate(zip(cols, similar_images[category].iterrows())):\n","                        filename = row['aihub_image']\n","                        image_path = os.path.join(image_directory, filename)\n","                        with col:\n","                            if os.path.exists(image_path):\n","                                img = Image.open(image_path)\n","                                img = ImageOps.exif_transpose(img)\n","                                img = img.resize((200, 200))\n","                                st.image(img, caption=f\"{i + 1}\", use_container_width=True)\n","                            else:\n","                                st.write(f\"{i + 1}. ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}\")\n","                            options.append((f\"{i + 1}\", filename))\n","                    # ë¼ë””ì˜¤ ë²„íŠ¼ì„ í†µí•´ ì´ë¯¸ì§€ ì„ íƒ, ê° ë¼ë””ì˜¤ì˜ í‚¤ëŠ” ì¹´í…Œê³ ë¦¬ì™€ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©í•´ ìœ ì¼í•˜ê²Œ ì„¤ì •\n","                    selected_option = st.radio(\n","                        \"ğŸ’–ë§ˆìŒì— ë“œëŠ” ì‚¬ì§„ì„ ê³¨ë¼ì£¼ì„¸ìš”.\",\n","                        [opt[0] for opt in options],\n","                        key = f\"{category}_radio_{i}\",\n","                        horizontal = True\n","                    )\n","                    # ì„ íƒëœ íŒŒì¼ëª…ì„ session_stateì— ì €ì¥\n","                    try:\n","                        st.session_state.selected_images[category] = next(\n","                            filename for label, filename in options if label == selected_option\n","                        )\n","                    except StopIteration:\n","                        st.write(\"ì„ íƒëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","                if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                    st.session_state.form_submitted = True\n","        message_placeholder.write(\"ëª¨ë“  ì„ íƒì„ ì™„ë£Œí–ˆì–´ìš”! ì´ì œ ìµœì¢… ì‚¬ì§„ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.\")\n","        # í¼ ì œì¶œ í›„ ìµœì¢… ìœ ì‚¬ ì´ë¯¸ì§€ ê³„ì‚°\n","        if st.session_state[\"form_submitted\"] and not st.session_state[\"profile_submitted\"]:\n","            # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± ë° í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n","            average_similarities = generate_similarity_info(\n","                    st.session_state.selected_images,\n","                    final_similarity_df_test,\n","                    similar_images\n","                )\n","            # ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n","            most_similar_image_path = get_most_similar_image_path(average_similarities)\n","            if most_similar_image_path and os.path.exists(most_similar_image_path):\n","                message_placeholder.write(\"ğŸ‰ê°€ì¥ ë¹„ìŠ·í•œ ì¶”ì–µì„ ì°¾ì•˜ì–´ìš”!\")\n","                most_similar_image = os.path.basename(most_similar_image_path)\n","                img = Image.open(most_similar_image_path)\n","                img = ImageOps.exif_transpose(img)\n","                img = img.resize((200, 220))\n","                st.image(img)\n","                # ì¡°ê±´ì— ë§ëŠ” recommend_table í–‰ ì¶”ì¶œ\n","                spot = recommend_table.loc[recommend_table['PHOTO_FILE_NM'] == most_similar_image]\n","                #st.write(\"ìµœì¢… ì‚¬ì§„ ì—¬í–‰ì§€ ì •ë³´:\", spot)\n","\n","                # ì‚¬ìš©ì íŠ¹ì„± ì…ë ¥ í¼\n","                travel_style_mapping = {\n","                    'ìì—° ë§¤ìš° ì„ í˜¸': 1,\n","                    'ìì—° ì¤‘ê°„ ì„ í˜¸': 2,\n","                    'ìì—° ì•½ê°„ ì„ í˜¸': 3,\n","                    'ì¤‘ë¦½': 4,\n","                    'ë„ì‹œ ì•½ê°„ ì„ í˜¸': 5,\n","                    'ë„ì‹œ ì¤‘ê°„ ì„ í˜¸': 6,\n","                    'ë„ì‹œ ë§¤ìš° ì„ í˜¸': 7\n","                }\n","\n","                travel_motive_mapping = {\n","                    'ì¼ìƒì ì¸ í™˜ê²½ ë° ì—­í• ì—ì„œì˜ íƒˆì¶œ, ì§€ë£¨í•¨ íƒˆí”¼': 1,\n","                    'ì‰´ ìˆ˜ ìˆëŠ” ê¸°íšŒ, ìœ¡ì²´ í”¼ë¡œ í•´ê²° ë° ì •ì‹ ì ì¸ íœ´ì‹': 2,\n","                    'ì—¬í–‰ ë™ë°˜ìì™€ì˜ ì¹œë°€ê° ë° ìœ ëŒ€ê° ì¦ì§„': 3,\n","                    'ì§„ì •í•œ ìì•„ ì°¾ê¸° ë˜ëŠ” ìì‹ ì„ ë˜ëŒì•„ë³¼ ê¸°íšŒ ì°¾ê¸°': 4,\n","                    'SNS ì‚¬ì§„ ë“±ë¡ ë“± ê³¼ì‹œ': 5,\n","                    'ìš´ë™, ê±´ê°• ì¦ì§„ ë° ì¶©ì „': 6,\n","                    'ìƒˆë¡œìš´ ê²½í—˜ ì¶”êµ¬': 7,\n","                    'ì—­ì‚¬ íƒë°©, ë¬¸í™”ì  ê²½í—˜ ë“± êµìœ¡ì  ë™ê¸°': 8,\n","                    'íŠ¹ë³„í•œ ëª©ì (ì¹ ìˆœì—¬í–‰, ì‹ í˜¼ì—¬í–‰, ìˆ˜í•™ì—¬í–‰, ì¸ì„¼í‹°ë¸Œì—¬í–‰)': 9,\n","                    'ê¸°íƒ€': 10\n","                }\n","\n","                st.subheader(\"ë³¸ì¸ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”\")\n","                with st.form(\"user_profile_form\"):\n","                    gender = st.radio(\"âœ…ì„±ë³„\", [0, 1], format_func=lambda x: \"ë‚¨ì„±\" if x == 0 else \"ì—¬ì„±\",\n","                                          horizontal=True)\n","                    age_group_label = st.radio(\"âœ…ë‚˜ì´\",\n","                     ['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€'], horizontal=True)\n","                    age_group = int(age_group_label[:-1])\n","                    travel_style_label = st.radio(\"âœ…ìì—° vs ë„ì‹œ\", list(travel_style_mapping.keys()))\n","                    travel_style = travel_style_mapping[travel_style_label]\n","                    travel_motive_label = st.radio(\"âœ…ì—¬í–‰ ë™ê¸°\", list(travel_motive_mapping.keys()))\n","                    travel_motive = travel_motive_mapping[travel_motive_label]\n","                    num_recommendations = st.slider(\"âœ…ì¶”ì²œë°›ê³  ì‹¶ì€ ì¥ì†Œ ê°œìˆ˜\", min_value=3, max_value=10, value=5, step=1)\n","\n","                    if st.form_submit_button(\"ì„ íƒ ì™„ë£Œ\"):\n","                        st.session_state.user_profile = {\n","                                \"GENDER\": gender,\n","                                \"AGE_GRP\": age_group,\n","                                \"TRAVEL_STYL_1\": travel_style,\n","                                \"TRAVEL_MOTIVE_1\": travel_motive,\n","                                \"NUM_RECOMMENDATIONS\": num_recommendations\n","                            }\n","                        st.session_state.profile_submitted = True\n","                        st.write(\"ğŸ’Œì •ë³´ê°€ ì €ì¥ë˜ì—ˆì–´ìš”!\")\n","                        st.session_state.new_traveler = np.array([gender, age_group, travel_style, travel_motive])\n","                        st.session_state.num_recommendations = num_recommendations\n","\n","        # ì¶”ì²œ ì¥ì†Œ ë° ì¶”ê°€ ì •ë³´ ì¶”ì¶œ\n","        if st.session_state.profile_submitted:\n","            locations = recommend(\n","                        spot,\n","                        st.session_state.new_traveler,\n","                        metric = 'knn',\n","                        n = st.session_state.num_recommendations,\n","                        recommend_with_consume = recommend_with_consume\n","                    )\n","            locations_time = locations_with_time(locations, act_consume)\n","            locations_time_print = locations_time[['VISIT_AREA_NM',\n","                                                   'ROAD_NM_ADDR',\n","                                                   'ratings']]\n","            locations_time_print.columns = ['ì´ë¦„', 'ì£¼ì†Œ', 'ë³„ì ']\n","            dining = extract_dining(locations_time, act_consume)\n","            st.subheader(\"âœ¨ì¶”ì²œ ì¥ì†Œ ì •ë³´\")\n","            st.table(locations_time_print)\n","\n","            # ê²½ë¡œ ìµœì í™” ìˆ˜í–‰\n","            path, duration, dist, duration_matrix, _ = path_optimizer(spot, locations)\n","\n","            # diningì„ MultiIndex DataFrameìœ¼ë¡œ ë³€í™˜\n","            dining_multiindex_df = convert_dining_to_multiindex(dining)\n","            poi_store_mapping = dining_multiindex_df.groupby('POI_NM')['STORE_NM'].apply(list).reset_index()\n","            st.subheader(\"âœ¨ë§›ì§‘ ì •ë³´\")\n","            poi_store_mapping_print = poi_store_mapping.copy()\n","            poi_store_mapping_print.columns = ['ì¶”ì²œ ì¥ì†Œ', 'ì£¼ë³€ ë§›ì§‘']\n","            st.table(poi_store_mapping_print)\n","\n","            # ê²½ë¡œì™€ ë§ˆì»¤ê°€ í¬í•¨ëœ ì§€ë„ ìƒì„±\n","            route_map = create_map_with_markers_and_ordered_paths(path, poi_store_mapping)\n","            # Streamlitì— Folium ì§€ë„ í‘œì‹œ\n","            #st.write(\"ìµœì  ê²½ë¡œ ì§€ë„\")\n","            #st_folium(route_map, width=700, height=500)\n","\n","if __name__=='__main__' :\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9de1f56-2090-4df3-e0ab-5787d8e85c17","executionInfo":{"status":"ok","timestamp":1731388954058,"user_tz":-540,"elapsed":585,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}},"id":"TwenMq6biIxG"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["# streamlit ì‹¤í–‰"],"metadata":{"id":"q1EgqfU-K_Cw"}},{"cell_type":"code","source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","\n","# \"Password/Enpoint IP for localtunnel is:\" ìš°ì¸¡ì— xx.xxx.xx.xxx í˜¹ì€ xx.xxx.xxx.xxx í˜•ì‹ì˜ ìˆ«ìê°€ ë‚˜ì˜¨ë‹¤.\n","\n","!streamlit run app.py &>/content/logs.txt &\n","!npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDUalmN3z1Th","outputId":"eadc923a-2211-40a5-9a0d-c9002598f9d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Password/Enpoint IP for localtunnel is: 34.126.107.92\n","your url is: https://lazy-dogs-suffer.loca.lt\n"]}]},{"cell_type":"code","source":["!npm audit fix --force"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyCNSzXAILqT","executionInfo":{"status":"ok","timestamp":1731377291776,"user_tz":-540,"elapsed":3908,"user":{"displayName":"ì´ì„¸ì€","userId":"02964283497915774514"}},"outputId":"6f869f07-32fa-4e65-b54d-26304819fffd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35musing --force\u001b[0m Recommended protections disabled.\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35maudit\u001b[0m Updating localtunnel to 1.8.3, which is a SemVer major change.\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m cryptiles@2.0.5: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m sntp@1.0.9: This module moved to @hapi/sntp. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m boom@2.10.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m har-validator@4.2.1: this library is no longer supported\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m hoek@2.16.3: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m request@2.81.0: request has been deprecated, see https://github.com/request/request/issues/3142\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m hawk@3.1.3: This module moved to @hapi/hawk. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n","\u001b[K\u001b[?25h\n","added 78 packages, removed 10 packages, changed 11 packages, and audited 91 packages in 3s\n","\n","11 packages are looking for funding\n","  run `npm fund` for details\n","\n","\u001b[1m# npm audit report\u001b[22m\n","\n","\u001b[1majv\u001b[22m  <6.12.3\n","Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n","\u001b[1mPrototype Pollution in Ajv\u001b[22m - https://github.com/advisories/GHSA-v88g-cgmw-v5xw\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@1.9.2, which is outside the stated dependency range\n","\u001b[2mnode_modules/ajv\u001b[22m\n","  \u001b[1mhar-validator\u001b[22m  3.3.0 - 5.1.0\n","  Depends on vulnerable versions of \u001b[1majv\u001b[22m\n","  \u001b[2mnode_modules/har-validator\u001b[22m\n","    \u001b[1mrequest\u001b[22m  *\n","    Depends on vulnerable versions of \u001b[1mhar-validator\u001b[22m\n","    Depends on vulnerable versions of \u001b[1mhawk\u001b[22m\n","    Depends on vulnerable versions of \u001b[1mtough-cookie\u001b[22m\n","    \u001b[2mnode_modules/request\u001b[22m\n","      \u001b[1mlocaltunnel\u001b[22m  <=1.9.0\n","      Depends on vulnerable versions of \u001b[1mdebug\u001b[22m\n","      Depends on vulnerable versions of \u001b[1mrequest\u001b[22m\n","      \u001b[2mnode_modules/localtunnel\u001b[22m\n","\n","\u001b[1mdebug\u001b[22m  <=2.6.8\n","Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n","\u001b[1mdebug Inefficient Regular Expression Complexity vulnerability\u001b[22m - https://github.com/advisories/GHSA-9vvw-cc9w-f27h\n","\u001b[1mRegular Expression Denial of Service in debug\u001b[22m - https://github.com/advisories/GHSA-gxpj-cx7g-858c\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@1.9.2, which is outside the stated dependency range\n","\u001b[2mnode_modules/debug\u001b[22m\n","\n","\u001b[1mhawk\u001b[22m  <=9.0.0\n","Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n","\u001b[1mUncontrolled Resource Consumption in Hawk\u001b[22m - https://github.com/advisories/GHSA-44pw-h2cw-w3vq\n","Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n","Depends on vulnerable versions of \u001b[1mcryptiles\u001b[22m\n","Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n","Depends on vulnerable versions of \u001b[1msntp\u001b[22m\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@1.9.2, which is outside the stated dependency range\n","\u001b[2mnode_modules/hawk\u001b[22m\n","\n","\u001b[1mhoek\u001b[22m  *\n","Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n","\u001b[1mPrototype Pollution in hoek\u001b[22m - https://github.com/advisories/GHSA-jp4x-w63m-7wgm\n","\u001b[1mhoek subject to prototype pollution via the clone function.\u001b[22m - https://github.com/advisories/GHSA-c429-5p7v-vgjp\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@1.9.2, which is outside the stated dependency range\n","\u001b[2mnode_modules/hoek\u001b[22m\n","  \u001b[1mboom\u001b[22m  <=3.1.2\n","  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n","  \u001b[2mnode_modules/boom\u001b[22m\n","    \u001b[1mcryptiles\u001b[22m  <=2.0.5\n","    Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n","    \u001b[2mnode_modules/cryptiles\u001b[22m\n","  \u001b[1msntp\u001b[22m  0.0.0 || 0.1.1 - 2.0.0\n","  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n","  \u001b[2mnode_modules/sntp\u001b[22m\n","\n","\n","\u001b[1mtough-cookie\u001b[22m  <4.1.3\n","Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n","\u001b[1mtough-cookie Prototype Pollution vulnerability\u001b[22m - https://github.com/advisories/GHSA-72xf-g2v4-qvf3\n","\u001b[33m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix --force`\n","Will install localtunnel@1.9.2, which is outside the stated dependency range\n","\u001b[2mnode_modules/tough-cookie\u001b[22m\n","\n","\u001b[31m\u001b[1m11\u001b[22m\u001b[39m vulnerabilities (3 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m, 8 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m)\n","\n","To address all issues, run:\n","  npm audit fix --force\n"]}]},{"cell_type":"markdown","source":["# streamlit ì½”ë© ì—°ë™ ë°©ë²• (ë…¸ê±´ë“¤)"],"metadata":{"id":"PW5bpydrz3Gs"}},{"cell_type":"code","source":["# 1. ìŠ¤íŠ¸ë¦¼ë¦¿ ì„¤ì¹˜\n","!pip install -q streamlit"],"metadata":{"id":"ZVzSPkOuz22Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2_ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë„ìš¸ ë‚´ìš©\n","\n","%%writefile app.py\n","\n","import streamlit as st\n","\n","st.write('Hello, *World!* :sunglasses:') # í•´ë‹¹ ë‚´ìš©ì„ ìˆ˜ì •í•´ì„œ ì‚¬ì´íŠ¸ë¥¼ ììœ ë¡­ê²Œ ê¾¸ë°€ ìˆ˜ ìˆë‹¤."],"metadata":{"id":"d90yBGEa0RvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3\n","import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","\n","# \"Password/Enpoint IP for localtunnel is:\" ìš°ì¸¡ì— xx.xxx.xx.xxx í˜¹ì€ xx.xxx.xxx.xxx í˜•ì‹ì˜ ìˆ«ìê°€ ë‚˜ì˜¨ë‹¤."],"metadata":{"id":"sVA1zPNq0WKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4\n","!npm install localtunnel"],"metadata":{"id":"VovGiW5i0jJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5\n","!streamlit run app.py &>/content/logs.txt &"],"metadata":{"id":"nFzT3j-90nWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6\n","!npx localtunnel --port 8501\n","\n","# \"your url is:\" ìš°ì¸¡ì— ì‚¬ì´íŠ¸ ì£¼ì†Œê°€ ìƒì„±ëœë‹¤."],"metadata":{"id":"22rpsQxR0pV_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["PW5bpydrz3Gs"],"gpuType":"A100","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}